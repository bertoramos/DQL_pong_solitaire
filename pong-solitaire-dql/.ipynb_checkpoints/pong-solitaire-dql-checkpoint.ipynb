{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del entorno PongSolitaire en Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import gym\n",
    "import gym_pong_solitaire\n",
    "\n",
    "env = gym.make('PongSolitaire-v0')\n",
    "\n",
    "reloj = pygame.time.Clock()\n",
    "\n",
    "a = env.reset()\n",
    "print(a)\n",
    "done = False\n",
    "while not done:\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render(mode='stat')\n",
    "    print(state)\n",
    "    reloj.tick(60)\n",
    "env.close()\n",
    "\n",
    "print(env.observation_space.low)  # x_ball, y_ball, dx, dy, x_paddle\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodio de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_pong_solitaire\n",
    "from agent import Agent\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('PongSolitaire-v0')\n",
    "\n",
    "agent = Agent(len(env.observation_space.low), env.action_space.n) # Toma decisiones de entrenamiento de la red neuronal resultante\n",
    "\n",
    "episodes = 10 # Numero de jugadas que se realizan\n",
    "time = 15_000 # Numero de estados por episodio\n",
    "replay_size = 32 # Tamaño del batch de replay (recordar experiencias pasadas)\n",
    "\n",
    "max_score = 0\n",
    "\n",
    "print(\"---------- Training start ----------\")\n",
    "\n",
    "for episode in range(episodes):\n",
    "    current_state = env.reset()\n",
    "    current_state = np.reshape(current_state, [1,5])\n",
    "\n",
    "    total_reward = 0\n",
    "    \n",
    "    print(\"-------------------------\\n\" +\n",
    "          \"Episode \" + str(episode) + \"/\" + str(episodes))\n",
    "    for t in range(time):\n",
    "        action = agent.act(current_state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        next_state = np.reshape(next_state, [1,5])\n",
    "\n",
    "        agent.remember(current_state, action, reward, next_state, done)\n",
    "        \n",
    "        if t%16 == 0 or reward > 0:\n",
    "            print(\"\\tTime \" + str(t) + \" Episode \" + str(episode) + \"/\" + str(episodes))\n",
    "        \n",
    "        current_state = next_state\n",
    "        total_reward += reward # La puntuación final es el número de veces que se consigue rebotar la pelota\n",
    "        \n",
    "        max_score = total_reward if max_score < total_reward else max_score\n",
    "        \n",
    "        if done:\n",
    "            print(\"\\tEpisode \" + str(episode) + \"/\" + str(episodes) + \" finished | Score: \" + str(total_reward) + \" | Max score: \" + str(max_score))\n",
    "            break\n",
    "    agent.replay(replay_size)\n",
    "\n",
    "env.close()\n",
    "print(\"---------- Training end ----------\")\n",
    "\n",
    "# SERIALIZACIÓN : salvar modelo\n",
    "agent.model.save('agent_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testeando la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recuperar la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "nn_model = load_model('agent_model.h5')\n",
    "print(nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testear comportamiento de la red en el ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "import gym_pong_solitaire\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('PongSolitaire-v0')\n",
    "\n",
    "current_state = env.reset()\n",
    "current_state = np.reshape(current_state, [1,5])\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "time = 0\n",
    "while not done:\n",
    "    prediction = nn_model.predict(current_state)\n",
    "    action = np.argmax(prediction)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    next_state = np.reshape(next_state, [1,5])\n",
    "    \n",
    "    time += 1\n",
    "    print(\"Time: \" + str(time) + \" | Reward: \" + str(reward) + \" | Total reward: \" + str(total_reward))\n",
    "\n",
    "    current_state = next_state\n",
    "    total_reward += reward # La puntuación final es el número de veces que se consigue rebotar la pelota\n",
    "    \n",
    "print(\"Total score: \" + str(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
